{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aec58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_PbwGQdOdihHTKSTYzhJc97NoNCT1oNe0kk9t8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4c84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "# Prompts\n",
    "pre_prompt = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
    "prompt_input = \"What is the capital of France?\"\n",
    "\n",
    "# Generate LLM response\n",
    "output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', # LLM model\n",
    "                        input={\"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \", # Prompts\n",
    "                        \"temperature\":0.1, \"top_p\":0.9, \"max_length\":128, \"repetition_penalty\":1})  # Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477a84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm here to help. The capital of France is Paris. Is there anything else I can help with? \n"
     ]
    }
   ],
   "source": [
    "full_response = \"\"\n",
    "\n",
    "for item in output:\n",
    "  full_response += item\n",
    "\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c1947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 23:07:03.335 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Travis\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import replicate\n",
    "\n",
    "# Set your API token (ensure this is stored securely in production)\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_PbwGQdOdihHTKSTYzhJc97NoNCT1oNe0kk9t8\"\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"LLM Chat Interface\")\n",
    "st.subheader(\"Powered by LLaMA13B-v2 Chat Model\")\n",
    "\n",
    "# User input\n",
    "pre_prompt = st.text_area(\"System Prompt\", \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\", height=150)\n",
    "user_input = st.text_input(\"User Prompt\", \"What is the capital of France?\")\n",
    "\n",
    "# Button to generate a response\n",
    "if st.button(\"Generate Response\"):\n",
    "    if user_input.strip() == \"\":\n",
    "        st.error(\"Please enter a prompt.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Call the model\n",
    "            st.info(\"Generating response...\")\n",
    "            output = replicate.run(\n",
    "                'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
    "                input={\n",
    "                    \"prompt\": f\"{pre_prompt} {user_input} Assistant: \",\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"max_length\": 128,\n",
    "                    \"repetition_penalty\": 1\n",
    "                }\n",
    "            )\n",
    "            # Combine the output chunks\n",
    "            full_response = \"\".join(output)\n",
    "            st.success(\"Response Generated:\")\n",
    "            st.text_area(\"Assistant Response\", full_response, height=200)\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dbc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
